{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNJMGgN2JP8u8U9MHD69x5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gunterdewinter/testrepo/blob/main/06_VIT_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VIT Vision Transformer Pytorch (encoder only)**\n",
        "\n",
        "https://github.com/tintn/vision-transformer-from-scratch/blob/main/vision_transformers.ipynb\n"
      ],
      "metadata": {
        "id": "t8jWlMo5_5aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "LF_jzIlfARD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DNQEhbou_the"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from pathlib import Path\n",
        "import os\n",
        "import zipfile\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download train and test data**"
      ],
      "metadata": {
        "id": "nWdkmKyXGUzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"faces\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"faces.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/gunterdewinter/testrepo/raw/main/data/faces.zip\")\n",
        "        print(\"Downloading faces data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"faces.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping faces data...\")\n",
        "        zip_ref.extractall(image_path)\n",
        "\n",
        "    # Remove .zip file\n",
        "    os.remove(data_path / \"faces.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vn6azpx6RUi",
        "outputId": "325ffef4-4e64-4d2e-c2a9-06f661efd8d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/faces directory exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create transform pipelines**"
      ],
      "metadata": {
        "id": "vxbyApm2GYuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomResizedCrop((128, 128), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
      ],
      "metadata": {
        "id": "4j2n0cKSDxKe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set train and test dir's**"
      ],
      "metadata": {
        "id": "owJt6MfBGfDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "3AFALMKXD1N7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create train + test datasets and classes**"
      ],
      "metadata": {
        "id": "kcO-5QBfGk_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "testset = torchvision.datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "classes = trainset.classes\n",
        "classes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEPw7IO5Fr-U",
        "outputId": "a232cdcc-4e73-4f49-92b5-44afdf756ee0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['man', 'woman']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create train + test dataloaders (batch)**"
      ],
      "metadata": {
        "id": "VRuUX6pDG3XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                            shuffle=True, num_workers=1)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                            shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "QX5RVUg5FZh5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Transformer Classes**"
      ],
      "metadata": {
        "id": "4we6B3CuHNRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian Error Linear Unit activation function**\n",
        "\n",
        "https://arxiv.org/abs/1606.08415\n",
        "\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
        "\n",
        "Used in Google BERT\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0b8BAChhCqBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGELUActivation(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))"
      ],
      "metadata": {
        "id": "wo7u8VaaHrZ2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PatchEmbeddings**\n",
        "\n",
        "Convert the image into patches and then project them into a vector space."
      ],
      "metadata": {
        "id": "PT-7-VIsIMNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.patch_size = config[\"patch_size\"]\n",
        "        self.num_channels = config[\"num_channels\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        # Calculate the number of patches from the image size and patch size\n",
        "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "        # Create a projection layer to convert the image into patches\n",
        "        # The layer projects each patch into a vector of size hidden_size\n",
        "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7fxyn-dkIUuL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**\n",
        "\n",
        "Combine the patch embeddings with the class token and position embeddings.\n",
        "  "
      ],
      "metadata": {
        "id": "qErPL2AMI16V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.patch_embeddings = PatchEmbeddings(config)\n",
        "        # Create a learnable [CLS] token\n",
        "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
        "        # and is used to classify the entire sequence\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
        "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
        "        # Add 1 to the sequence length for the [CLS] token\n",
        "        self.position_embeddings = \\\n",
        "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embeddings(x)\n",
        "        batch_size, _, _ = x.size()\n",
        "        # Expand the [CLS] token to the batch size\n",
        "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
        "        # This results in a sequence length of (num_patches + 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.position_embeddings\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6uHYPv1mI2fM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attentionhead**\n",
        "\n",
        "A single attention head.\n",
        "    This module is used in the MultiHeadAttention module."
      ],
      "metadata": {
        "id": "NKHcwp8eJVnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention_head_size = attention_head_size\n",
        "        # Create the query, key, and value projection layers\n",
        "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project the input into query, key, and value\n",
        "        # The same input is used to generate the query, key, and value,\n",
        "        # so it's usually called self-attention.\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "        # Calculate the attention scores\n",
        "        # softmax(Q*K.T/sqrt(head_size))*V\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        # Calculate the attention output\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "        return (attention_output, attention_probs)"
      ],
      "metadata": {
        "id": "43z0_rniJWVp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MultiHeadAttention**\n",
        "\n",
        "Multi-head attention module.\n",
        "This module is used in the TransformerEncoder module.\n"
      ],
      "metadata": {
        "id": "sWHb_LVUKZ25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a list of attention heads\n",
        "        self.heads = nn.ModuleList([])\n",
        "        for _ in range(self.num_attention_heads):\n",
        "            head = AttentionHead(\n",
        "                self.hidden_size,\n",
        "                self.attention_head_size,\n",
        "                config[\"attention_probs_dropout_prob\"],\n",
        "                self.qkv_bias\n",
        "            )\n",
        "            self.heads.append(head)\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Calculate the attention output for each attention head\n",
        "        attention_outputs = [head(x) for head in self.heads]\n",
        "        # Concatenate the attention outputs from each attention head\n",
        "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
        "        # Project the concatenated attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        # Return the attention output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
        "            return (attention_output, attention_probs)"
      ],
      "metadata": {
        "id": "Q2Hkh0d9KaH5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FasterMultiHeadAttention**\n",
        "\n",
        "Multi-head attention module with some optimizations.\n",
        "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
        "\n"
      ],
      "metadata": {
        "id": "fgJqodJCOrwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a linear layer to project the query, key, and value\n",
        "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
        "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, output_attentions=False):\n",
        "        # Project the query, key, and value\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
        "        qkv = self.qkv_projection(x)\n",
        "        # Split the projected query, key, and value into query, key, and value\n",
        "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
        "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
        "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        batch_size, sequence_length, _ = query.size()\n",
        "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        # Calculate the attention scores\n",
        "        # softmax(Q*K.T/sqrt(head_size))*V\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        # Calculate the attention output\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "        # Resize the attention output\n",
        "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        # To (batch_size, sequence_length, all_head_size)\n",
        "        attention_output = attention_output.transpose(1, 2) \\\n",
        "                                           .contiguous() \\\n",
        "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
        "        # Project the attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        # Return the attention output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            return (attention_output, attention_probs)"
      ],
      "metadata": {
        "id": "Fwhg6rOGKXWD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}